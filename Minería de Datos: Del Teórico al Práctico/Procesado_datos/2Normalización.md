Normalización es como usar la misma regla para medir cosas que, a primera vista, parecen muy diferentes, para que puedas compararlas más justamente. Imagina que quieres comparar la edad y el salario de unas personas; si las miras directamente, el salario, que se cuenta por miles, parecerá tener mucha más variación y peso que la edad, que apenas llega a las decenas. Esto puede hacer que, al analizar datos o al construir modelos como redes neuronales, termines dándole más importancia al salario solo porque sus números son más grandes, no porque realmente sea más relevante.

Para evitar esto, la normalización ajusta estos valores a una escala común, asegurándose de que las diferencias se entiendan de manera equitativa. Por ejemplo, si la diferencia de edad entre dos personas es de 30 años y la diferencia de su salario es de 300 euros, sin normalizar, el salario parece tener un cambio 10 veces mayor que la edad. Pero esto no es justo, porque estamos comparando manzanas con naranjas: años vs. euros. Al normalizar, ajustamos ambos a una escala donde podemos ver más claramente qué tan significativas son realmente esas diferencias, sin que nos confunda el simple tamaño de los números.

Tenemos dos formas de normalizar los datos:
[[Normalización por el máximo]], [[Normalización por la diferencia]], [[Escalado decimal]], [[Normalización basada en la desviación estándar]], 